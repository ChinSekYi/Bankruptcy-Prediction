{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Company Bankruptcy using Machine Learning\n",
    "By CS3244 Project Team 11  \n",
    "Cao Han  |  Chin Sek Yi  |  Lim Kai Sin  |  Luo Xinming  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Chapter 1: Project Overview](#chapter1)\n",
    "    * [1.1 Project Motivation](#section1_1)\n",
    "    * [1.2 Dataset Description](#section1_2)\n",
    "    * [1.3 Methodologies](#section1_3)\n",
    "* [Chapter 2: Data Preparation](#chapter2)\n",
    "    * [2.1 Data Collection](#section2_1)\n",
    "    * [2.2 Data Exploration (EDA)](#section2_2)\n",
    "    * [2.3 Data Pre-Processing](#section2_3)\n",
    "* [Chapter 3: Modelling](#chapter3)\n",
    "    * [3.1 Evaluation Metrics](#section3_1)\n",
    "    * [3.2 Machine Learning Models](#section3_2)\n",
    "    * [3.3 Our Modelling Approach](#section3_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1: Project Overview <a id=\"chapter1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Project Motivation <a id=\"section1_1\"></a>\n",
    "In today's dynamic business landscape, exemplified by recent events such as the bankruptcy of Silicon Valley Bank, the ability to anticipate and mitigate financial risks is crucial for sustainable growth and stability. This project aims to develop a robust predictive model of company bankruptcy, leveraging advanced machine learning algorithms and financial data analysis techniques, so as to equip stakeholders with nuanced insights to confidently traverse the unpredictable landscape of financial risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Dataset Description <a class=\"anchor\" id=\"section1_2\"></a>\n",
    "The dataset used in this project is about bankruptcy prediction of Polish companies. The dataset contains financial rates from one year and corresponding class label that indicates bankruptcy status 3 years after that year. The data contains 10503 instances (financial statements), with 495 representing bankrupted companies and 10008 still operating at the end of the 3-year forecasting period.\n",
    "\n",
    "The data was collected from [Emerging Markets Information Service](http://www.securities.com), which is a database containing information on emerging markets around the world. The bankrupt companies were analyzed in the period 2000-2012, while the still operating companies were evaluated from 2007 to 2013.  \n",
    "\n",
    "Source: [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/dataset/365/polish+companies+bankruptcy+data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Methodologies <a class=\"anchor\" id=\"section1_3\"></a>\n",
    "This project is structured into five distinct steps:\n",
    "\n",
    "1. **Data Collection**: This initial phase involves setting up a web scraper to automate the acquisition and transformation of the desired dataset. The process is designed to systematically download and convert data, streamlining subsequent analysis.\n",
    "\n",
    "2. **Exploratory Data Analysis (EDA)**: In this step, various visualization tools such as histograms and heatmaps are employed to explore the data distribution and examine the relationships between features. This analysis helps identify patterns and insights that inform further data handling strategies.\n",
    "\n",
    "3. **Data Pre-processing**: During this stage, missing values are addressed through mean imputation, ensuring no data is unnecessarily discarded. Additionally, numerical values are standardized to neutralize disparities in scale among features, enhancing model accuracy. To mitigate the effects of multicollinearity, features that show high correlation with others are selectively removed. As the dataset is highly imbalanced, we used Synthetic Minority Over-sampling Technique (SMOTE) to generate instances of the minortity class for training. \n",
    "\n",
    "4. **Modeling**: Various well-established machine learning algorithms are utilized in this step, including Support Vector Machines (SVM), K-Nearest Neighbors (KNN), and Decision Trees, to develop predictive models for predicting company bankruptcy. Logistic regression serves as a baseline for comparison. Advanced techniques such as bagging and boosting are implemented to improve each model's robustness. Furthermore, the potential of neural networks is explored to assess their efficacy in enhancing predictive performance.\n",
    "\n",
    "5. **Evaluation and Comparison**: The final step involves a thorough evaluation and comparison of each model's performance. The outcomes are meticulously analyzed to identify the most effective model, which is then detailed in this section, highlighting the relative advantages and effectiveness of the approaches used.\n",
    "\n",
    "This structured approach ensures a comprehensive analysis and robust development of predictive models, aiming to deliver reliable and actionable insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: Data Preparation <a class=\"anchor\" id=\"chapter2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data Collection <a class=\"anchor\" id=\"section2_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "from scipy.io import arff\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_url = \"https://archive.ics.uci.edu/static/public/365/polish+companies+bankruptcy+data.zip\"\n",
    "response = requests.get(download_url)\n",
    "zip_file_path = \"downloaded_file.zip\"\n",
    "\n",
    "with open(zip_file_path, \"wb\") as file:\n",
    "    file.write(response.content)\n",
    "\n",
    "def extract_arff_from_zip(zip_path, arff_filename, extraction_path='.'):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extract(arff_filename, path=extraction_path)\n",
    "        return os.path.join(extraction_path, arff_filename)\n",
    "\n",
    "def convert_arff_to_csv(arff_path, csv_path):\n",
    "    data, meta = arff.loadarff(arff_path)\n",
    "    df = pd.DataFrame(data)    \n",
    "    for col in df.select_dtypes([object]):\n",
    "        if isinstance(df[col][0], bytes):\n",
    "            df[col] = df[col].apply(lambda x: x.decode('utf-8'))\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "for i in range(1, 6):\n",
    "    arff_filename = f'{i}year.arff'\n",
    "    csv_path = f'{i}year.csv'\n",
    "    # extracted_arff_path = extract_arff_from_zip(zip_file_path, arff_filename)\n",
    "    # convert_arff_to_csv(extracted_arff_path, csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data Exploration (EDA) <a class=\"anchor\" id=\"section2_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Visualization <a id=\"section2_2_1\"></a> \n",
    "In this stage, we examined the feature columns and their data types to gain a comprehensive understanding of the dataset, including its size, structure, and characteristics. By visualizing the data through various plots and charts, data exploration enables the detection of patterns, trends, and relationships between different variables. This can provide valuable insights into the underlying relationships within the data.\n",
    "\n",
    "We use histograms, box plots, and density plots to visually explore the distribution of each feature. By using scatter plots and heat maps, we explore the correlation and relationship within each pair of features to ease the feature selection process.\n",
    "\n",
    "Exploring the data allows for the assessment of data quality, including issues such as data inconsistencies, errors, or biases. This ensures that the data used for analysis is reliable and representative of the underlying phenomena.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Detection of Outliers<a id=\"section2_2_2\"></a>\n",
    "Outliers, or data points that significantly deviate from the rest of the dataset, can have a significant impact on model performance. Data exploration techniques help in identifying outliers and deciding on appropriate actions, such as removing them or transforming them to mitigate their influence on the analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Data Pre-Processing <a class=\"anchor\" id=\"section2_3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: Modelling <a class=\"anchor\" id=\"chapter3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Evaluation Metrics  <a class=\"anchor\" id=\"section3_1\"></a>\n",
    "\n",
    "Our primary task is to predict bankruptcy (a classification problem). Below are the key classification metrics that we will use to evaluate our model performance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Accuracy\n",
    "Accuracy provides a ratio of correctly predicted observations to the total observations. It is especially useful when the classes are balanced with SMOTE.\n",
    "\n",
    "**Formula**:\n",
    "$$\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Confusion Matrix and Related Terms\n",
    "The confusion matrix is a table layout that allows visualization of the performance of the algorithm, where each number in the matrix represents:\n",
    "- **TP (True Positives)**: Correctly predicted positive observations.\n",
    "- **TN (True Negatives)**: Correctly predicted negative observations.\n",
    "- **FP (False Positives)**: Incorrectly predicted as positive.\n",
    "- **FN (False Negatives)**: Incorrectly predicted as negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 Precision, Recall, and F1-Score\n",
    "These metrics offer a deeper understanding of the model's performance by taking into account data imbalances, thereby providing a more nuanced view of the accuracy across different class labels.\n",
    "\n",
    "- **Precision**:\n",
    "$$ \\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}} $$\n",
    "Precision measures the accuracy of positive predictions.\n",
    "\n",
    "- **Recall** (or Sensitivity or TPR):\n",
    "$$ \\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} $$\n",
    "Recall measures the ability of a model to find all the relevant cases (all positive samples).\n",
    "\n",
    "- **F1-Score**:\n",
    "$$ \\text{F1-Score} = 2 \\times \\left( \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} \\right) $$\n",
    "The F1-Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. A high F1-score shows a model can classify the positive class correctly, while not misclassifying many negative classes as positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.4 Macro Average vs Micro Average\n",
    "\n",
    "- **Macro Average**: Macro average calculates the metric independently for each class and then takes the average (hence treating all classes equally). It is useful equal weight is given to the performance of each class, regardless of its frequency.\n",
    "\n",
    "- **Micro Average**: Micro average aggregates the contributions of all classes to compute the average metric. In other words, Micro Average will compute the overall total TP, FP, and FN across all classes, and then use these totals to calculate the performance scores. It is useful when metrics are weighted by class size, which is ideal for imbalanced data as it reflects the contribution of each class proportionally to its size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import evaluation metrics\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             confusion_matrix, f1_score, precision_score,\n",
    "                             recall_score)\n",
    "\n",
    "# function to print all evaluation results\n",
    "def get_acc(y_test, y_pred):\n",
    "    return round(accuracy_score(y_test, y_pred), 3)\n",
    "\n",
    "\n",
    "def get_pre(y_test, y_pred):\n",
    "    return round(precision_score(y_test, y_pred), 3)\n",
    "\n",
    "\n",
    "def get_rec(y_test, y_pred):\n",
    "    return round(recall_score(y_test, y_pred), 3)\n",
    "\n",
    "\n",
    "def get_f1(y_test, y_pred):\n",
    "    return round(f1_score(y_test, y_pred), 3)\n",
    "\n",
    "def print_res(y_test, y_pred):\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(\"Accuracy:\", get_acc(y_test, y_pred))\n",
    "    print(\"Precision Score:\", get_pre(y_test, y_pred))\n",
    "    print(\"Recall Score:\", get_rec(y_test, y_pred))\n",
    "    print(\"F1 Score:\", get_f1(y_test, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Machine Learning Model Packages\n",
    "\n",
    "#### Scikit-learn (sklearn)\n",
    "- **KNeighborsClassifier**: This model implements the k-nearest neighbors voting algorithm.\n",
    "- **LogisticRegression**: A model that applies logistic regression for binary classification tasks.\n",
    "- **SVC**: Support Vector Machine classifier known for its effectiveness in high-dimensional spaces.\n",
    "- **DecisionTreeClassifier**: A model that uses a decision tree for classification, useful for interpretability.\n",
    "- **GradientBoostingClassifier**: An ensemble model that builds on weak prediction models to create a strong classifier.\n",
    "- **RandomForestClassifier**: A meta estimator that fits a number of decision tree classifiers to various sub-samples of the dataset and uses averaging to improve predictive accuracy and control overfitting.\n",
    "\n",
    "#### Imbalanced-learn (imblearn)\n",
    "Dealing with imbalanced data:\n",
    "\n",
    "- **BalancedRandomForestClassifier**: A variation of the RandomForest that handles imbalances by adjusting weights inversely proportional to class frequencies in the input data.\n",
    "\n",
    "#### XGBoost (xgboost)\n",
    "\n",
    "- **XGBoost**: An implementation of gradient boosted decision trees designed for speed and performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "\n",
    "# imblearn\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Model Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1 Simple logistic regression\n",
    "We begin our modeling with a simple logistic regression to establish a baseline for benchmark performance.\n",
    "This approach provides an initial overview of how well simple models can predict outcomes based on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2 K-Nearest Neighbors (KNN)\n",
    "We continue our modeling with the K-Nearest Neighbors (KNN) algorithm to compare its performance against the baseline established by logistic regression. KNN is a technique that predicts the class of a given point based on the majority vote of its nearest neighbors. KNN is useful for gaining insights into the dataset’s structure due to its reliance on feature similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the high dimensionality of our dataset, even after data cleaning, the KNN model might not perform optimally due to its known limitations with high-dimensional data. To address this issue, we apply Principal Component Analysis (PCA) to reduce the dimensionality. This step aims to enhance the performance of our KNN model by focusing on the most significant features and reducing the noise associated with less important variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn with pca code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.3 Support Vector Machine (SVM)\n",
    "We further our analysis by incorporating a Support Vector Machine (SVM) model to assess its effectiveness compared to the simpler models previously tested. SVM is a classification technique that finds the optimal hyperplane which best separates the data into different classes. It addresses the short comings of KNN, as it performs well in high-dimensional space and work well with non-linearly separable data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    precision_score,\n",
    ")\n",
    "\n",
    "def SVM_return_model(*args, kernel_type_):\n",
    "    X_train = args[0]\n",
    "    X_test = args[1]\n",
    "    y_train = args[2]\n",
    "    y_test = args[3]\n",
    "\n",
    "    # Reset indices to ensure alignment\n",
    "    X_train.reset_index(drop=True, inplace=True)\n",
    "    y_train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Training the SVM model\n",
    "    svm_model = SVC(\n",
    "        kernel= kernel_type_\n",
    "    )  \n",
    "    print(f\"\\nkernel_type: {kernel_type_}\")\n",
    "    svm_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions on the testing set\n",
    "    y_pred_train = svm_model.predict(X_train)\n",
    "    y_pred_test = svm_model.predict(X_test)\n",
    "\n",
    "    # Evaluating the model\n",
    "    train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    precision_score_ = precision_score(y_test, y_pred_test)\n",
    "    recall_score_ = recall_score(y_test, y_pred_test)\n",
    "    f1_score_ = f1_score(y_test, y_pred_test)\n",
    "\n",
    "    #print(classification_report(y_test, y_pred_test))\n",
    "    print(f\"train_accuracy: {train_accuracy}\")\n",
    "    print(f\"test_accuracy: {test_accuracy}\")\n",
    "    print(f\"precision_score: {precision_score_}\")\n",
    "    print(f\"recall_score: {recall_score_}\")\n",
    "    print(f\"f1_score: {f1_score_}\")\n",
    "\n",
    "    return svm_model\n",
    "\n",
    "def SVM_sigmoid_model(*args): #for ANOVA\n",
    "    X_train = args[0]\n",
    "    X_test = args[1]\n",
    "    y_train = args[2]\n",
    "    y_test = args[3]\n",
    "\n",
    "    # Reset indices to ensure alignment\n",
    "    X_train.reset_index(drop=True, inplace=True)\n",
    "    y_train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    model = SVM_return_model(*args, kernel_type_=\"sigmoid\")\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "\n",
    "    # Evaluating the model\n",
    "    train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "    return train_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_dataset = dp.pre_process(df)\n",
    "\n",
    "kernel_list = [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\n",
    "kernel_dict = dict()\n",
    "\n",
    "for kernel_type in kernel_list:\n",
    "    print(f\"\\033[96m{kernel_type}\\033[00m\")\n",
    "    model = SVM_return_model(*train_test_dataset, kernel_type_=kernel_type)\n",
    "\n",
    "# we found that sigmoid gives the best test accuracy\n",
    "best_kernel_type = \"sigmoid\"\n",
    "\n",
    "best_train_test_dataset = dp.find_best_k_features_from_ANOVA(\n",
    "    SVM_sigmoid_model, *train_test_dataset\n",
    ")\n",
    "\n",
    "# Now we create a SVM model based on the top 25 features after ANOVA test\n",
    "SVM_model2 = SVM_return_model(*best_train_test_dataset, kernel_type_=best_kernel_type)\n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1 = best_train_test_dataset\n",
    "conf_matrix = confusion_matrix(y_test1, y_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Support Vector Machine (SVM) model has not performed as expected in our analysis for several reasons:\n",
    "\n",
    "1. **Sensitivity to Outliers**: SVM is particularly sensitive to outliers. In our dataset, we retained outliers to capture distinctive characteristics of different companies with respect to their bankruptcy status. This sensitivity can lead to skewed decision boundaries, adversely affecting model performance. \n",
    "\n",
    "2. **Overfitting Issues**: In this analysis, the SVM model has shown a tendency to overfit. This issue stems from the lack of an appropriate regularization function. Without proper regularization, SVM models can overly conform to the noise in the training data rather than capturing the general pattern, leading to poor generalization on new, unseen data.\n",
    "\n",
    "Hence, this prompt us to improve our SVM model with grid search method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"C\": [0.1, 1, 10, 100, 1000],\n",
    "    \"gamma\": [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "    \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\n",
    "}\n",
    "\n",
    "# grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=3)\n",
    "grid = GridSearchCV(SVM_model2, param_grid, refit=True, verbose=3)\n",
    "\n",
    "# fitting the model for grid search\n",
    "grid.fit(X_train1, y_train1)\n",
    "\n",
    "# print best parameter after tuning\n",
    "print(grid.best_params_)\n",
    "\n",
    "# print how our model looks after hyper-parameter tuning\n",
    "print(grid.best_estimator_)\n",
    "\n",
    "grid_predictions = grid.predict(X_test1)\n",
    "\n",
    "# print classification report (without grid search)\n",
    "print(\"SVM model without grid-search\")\n",
    "y_pred_test = SVM_model2.predict(X_test1)\n",
    "print(confusion_matrix(y_test1, y_pred_test))\n",
    "clf = SVM_return_model(*best_train_test_dataset, best_kernel_type)  # to print accuracy score\n",
    "\n",
    "# print classification report with grid search\n",
    "print(\"\\nSVM model with grid-search\")\n",
    "print(confusion_matrix(y_test1, grid_predictions))\n",
    "print(classification_report(y_test1, grid_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.4 Decision Tree\n",
    "A Decision Tree  works by splitting the dataset into smaller subsets based on feature values, thereby creating a tree structure. Each split is strategically made to maximize information gain—the reduction in entropy or disorder after the split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "print_res(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With grid search, we found the Best parameters: {'max_depth': 40, 'max_leaf_nodes': None, 'min_samples_leaf': 1, 'min_samples_split': 2}.\n",
    "We further explore the model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(max_depth = 40,\n",
    "                            max_leaf_nodes= None,\n",
    "                            min_samples_leaf= 1,\n",
    "                            min_samples_split= 2)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "print_res(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.5 Random Forest\n",
    "Random forest method is introduced as an improvement over single decision trees to reduce overfitting and improve predictive performance. Two underlying principles of random forest are:\n",
    "1. **Bagging**: It creates multiple subsets of the training data through random sampling with replacement and using these subsets to train a collection of decision trees (weak learners). This method aims to reduce variance of prediction result.\n",
    "2. **Random Subspace**: The random forest randomly selects a subset of features  from the original feature set for each individual decision tree within the ensemble. By allowing each tree to focus on a different subset of features, the random forest can capture different patterns and relationships within the data, leading to more accurate and stable predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest initial trial\n",
    "rdt = RandomForestClassifier(n_estimators=100, max_depth=5,random_state=42)\n",
    "\n",
    "rdt.fit(X_train, y_train)\n",
    "y_pred = rdt.predict(X_test)\n",
    "\n",
    "print_res(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest with grid search best model\n",
    "rdt = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_features='sqrt',\n",
    "    min_samples_leaf=4,  # Increased from 1 to 4\n",
    "    min_samples_split=10,  # Increased from 2 to 10\n",
    "    max_depth=25,  \n",
    "    class_weight= {0:0.7, 1:0.32},\n",
    "    bootstrap=True,\n",
    "    min_impurity_decrease=0.00001,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rdt.fit(X_train, y_train)\n",
    "y_pred = rdt.predict(X_test)\n",
    "\n",
    "print_res(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the model fitting process, we found out that the model could overfit on the training data, hence we proceed to reduce the dimension of our data with feature selection using forward selection loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest with forward selection\n",
    "selected_features = []  \n",
    "best_f1 = 0  \n",
    "features = list(X_train.columns)\n",
    "\n",
    "# Forward Selection\n",
    "for _ in range(len(features)):\n",
    "    f1_scores = []\n",
    "    for feature in features:\n",
    "        if feature not in selected_features:\n",
    "            temp_features = selected_features + [feature]\n",
    "            rdt = RandomForestClassifier(\n",
    "                n_estimators=100,\n",
    "                max_features='sqrt',\n",
    "                min_samples_leaf=4,  # Increased from 1 to 4\n",
    "                min_samples_split=10,  # Increased from 2 to 10\n",
    "                max_depth=25,  \n",
    "                class_weight= {0:0.7, 1:0.32},\n",
    "                bootstrap=True,\n",
    "                min_impurity_decrease=0.00001,\n",
    "                random_state=42\n",
    "            )\n",
    "            rdt.fit(X_train[temp_features], y_train)\n",
    "            y_pred = rdt.predict(X_test[temp_features])\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            f1_scores.append((feature, f1))\n",
    "    \n",
    "    f1_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    best_feature, best_feature_f1 = f1_scores[0]\n",
    "\n",
    "    if best_feature_f1 > best_f1:\n",
    "        print(f\"Adding {best_feature} improved F1 to {best_feature_f1}\")\n",
    "        best_f1 = best_feature_f1\n",
    "        selected_features.append(best_feature)\n",
    "    else:\n",
    "        break  \n",
    "\n",
    "print(\"Selected features:\", selected_features)\n",
    "\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_test_selected = X_test[selected_features]\n",
    "\n",
    "rdt_selected = RandomForestClassifier(n_estimators=200, max_features='sqrt', min_samples_leaf=1, min_samples_split=2)\n",
    "rdt_selected.fit(X_train_selected, y_train)\n",
    "y_pred_selected = rdt_selected.predict(X_test_selected)\n",
    "\n",
    "print_res(y_test, y_pred_selected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another supervised learning method we explored is Support Vector Machine (SVM). It works by finding the hyperplane that best separates different classes in the feature space, maximising the margin between classes. It is effective for both linearly separable and non-linearly separable data through the use of kernel functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Numerical results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
