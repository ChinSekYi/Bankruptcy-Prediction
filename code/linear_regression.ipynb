{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\"\"\"\n",
    "functions starting with df_ can generate a processed dataframe directly\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def as_discrete(col):\n",
    "    n = len(col)\n",
    "    new_col = [0] * n\n",
    "    for i in range(n):\n",
    "        if col[i] == b\"0\":\n",
    "            new_col[i] = 0\n",
    "        else:\n",
    "            new_col[i] = 1\n",
    "    return pd.DataFrame(new_col)\n",
    "\n",
    "\n",
    "def get_Xy(df):\n",
    "    X = df.iloc[:, 0 : len(df) - 1]\n",
    "    y = as_discrete(df.iloc[:, -1])\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def med_impute(df, y):\n",
    "    # remove columns with more than 40% values being null\n",
    "    thd1 = df.shape[0] * 0.4\n",
    "    cols = df.columns[df.isnull().sum() < thd1]\n",
    "    df = df[cols]\n",
    "\n",
    "    # remove rows with more than 50% values being null\n",
    "    thd2 = df.shape[1] * 0.5\n",
    "    y = y[df.isnull().sum(axis=1) <= thd2]\n",
    "    df = df[df.isnull().sum(axis=1) <= thd2]\n",
    "\n",
    "    # median imputation for null values\n",
    "    df = df.fillna(df.median())\n",
    "\n",
    "    return df, y\n",
    "\n",
    "\n",
    "def normalise(df):\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "    return X_scaled\n",
    "\n",
    "\n",
    "def df_null_removal(df):\n",
    "    # Extract features (X) and target (y)\n",
    "    X, y = get_Xy(df)\n",
    "\n",
    "    # Remove null values and impute missing values\n",
    "    X_imputed, y = med_impute(X, y)\n",
    "\n",
    "    # Scale the imputed data\n",
    "    X_scaled_df = normalise(X_imputed)\n",
    "\n",
    "    return pd.concat([X_scaled_df, y], axis=1)\n",
    "\n",
    "\n",
    "def drop_high_corr(df, threshold=0.7):\n",
    "    correlation_matrix = df.corr()\n",
    "    high_cor = []\n",
    "    dropped_features = []\n",
    "\n",
    "    # Iterate through the correlation matrix to find highly correlated pairs\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(correlation_matrix.iloc[i, j]) > threshold:\n",
    "                if correlation_matrix.columns[j] != correlation_matrix.columns[i]:\n",
    "                    high_cor.append(\n",
    "                        [\n",
    "                            correlation_matrix.columns[i],\n",
    "                            correlation_matrix.columns[j],\n",
    "                            correlation_matrix.iloc[i, j],\n",
    "                        ]\n",
    "                    )\n",
    "\n",
    "    # Iterate through the list of highly correlated pairs\n",
    "    for pair in high_cor:\n",
    "        feature1, feature2, correlation = pair\n",
    "\n",
    "        # Check if either of the features in the pair has already been dropped\n",
    "        if feature1 not in dropped_features and feature2 not in dropped_features:\n",
    "            # Check if the feature exists in the DataFrame before attempting to drop it\n",
    "            if feature2 in df.columns:\n",
    "                # Drop one of the correlated features from the dataset\n",
    "                # Here, we arbitrarily choose to drop the second feature in the pair\n",
    "                df.drop(feature2, axis=1, inplace=True)\n",
    "                dropped_features.append(feature2)\n",
    "            else:\n",
    "                #print(f\"Feature {feature2} not found in the DataFrame.\")\n",
    "                print(\"Feature '\" + feature2 + \"' not found in the DataFrame.\") #temporary \n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def df_null_corr_process(df):\n",
    "    X, y = df_null_removal(df)\n",
    "    return drop_high_corr(X), y\n",
    "\n",
    "\n",
    "def pre_process(df):\n",
    "    X, y = get_Xy(df)\n",
    "    X_imputed, y_final = med_impute(X, y)\n",
    "    X_scaled = normalise(X_imputed)\n",
    "    X_final = drop_high_corr(X_scaled)\n",
    "\n",
    "    return X_final, y_final\n",
    "\n",
    "\n",
    "def get_train_test(df):\n",
    "\n",
    "    X, y = get_Xy(df)\n",
    "    X_imputed, y_final = med_impute(X, y)\n",
    "    X_scaled = normalise(X_imputed)\n",
    "    X_final = drop_high_corr(X_scaled)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_final, y_final, test_size=0.2, random_state=3244\n",
    "    )\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.io import arff\n",
    "import data_processing\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "data = arff.loadarff(\"../data/3year.arff\")\n",
    "df = pd.DataFrame(data[0])\n",
    "df_origin = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7035, 31)\n",
      "162\n"
     ]
    }
   ],
   "source": [
    "df_x, df_y = pre_process(df)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.33, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "z = y_test[0].sum()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10503, 65)\n",
      "(7035, 25)\n",
      "(7035, 1)\n",
      "(3466, 25)\n",
      "(3466, 1)\n"
     ]
    }
   ],
   "source": [
    "def get_df_with_top_k_features(k_features, X_train, X_test, y_train, y_test):\n",
    "    # define feature selection\n",
    "    fs = SelectKBest(score_func=f_classif, k=k_features)\n",
    "\n",
    "    # apply feature selection\n",
    "    X_selected = fs.fit_transform(X_train, y_train)\n",
    "\n",
    "    # Take the features with the highest F-scores\n",
    "    fs_scores_array = np.array(fs.scores_)\n",
    "\n",
    "    # Get the indices that would sort the array in descending order\n",
    "    sorted_indices_desc = np.argsort(fs_scores_array)[::-1]\n",
    "\n",
    "    # Take the top k indices\n",
    "    top_indices = sorted_indices_desc[:k_features]\n",
    "\n",
    "    X_train_with_selected_x = X_train.iloc[:, top_indices]\n",
    "    X_test_with_selected_x = X_test.iloc[:, top_indices]\n",
    "    return X_train_with_selected_x, X_test_with_selected_x, y_train, y_test\n",
    "\n",
    "k_features= 25\n",
    "X_train, X_test, y_train, y_test  = get_df_with_top_k_features(k_features, X_train, X_test, y_train, y_test)\n",
    "print(df.shape)\n",
    "print(X_train.shape) #dropped 2 rows after preprocessing?\n",
    "print(y_train.shape)\n",
    "print(X_test.shape) #dropped 2 rows after preprocessing?\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7035, 25)\n",
      "<statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x131496dd0>\n",
      "Number of attributes that are not significant: 25 / 25\n",
      "MSE: 0.04510058667222804\n"
     ]
    }
   ],
   "source": [
    "# Linear regression - test significance\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def linear_regression_model(X_train, y_train):\n",
    "    # Reset indices to ensure alignment\n",
    "    X_train.reset_index(drop=True, inplace=True)\n",
    "    y_train.reset_index(drop=True, inplace=True)\n",
    "    print(X_train.shape)\n",
    "\n",
    "    # Add constant column to the features\n",
    "    x_features = sm.add_constant(X_train)\n",
    "\n",
    "    # Fit OLS model\n",
    "    ols_model = sm.OLS(y_train, x_features)\n",
    "    fit_results = ols_model.fit()\n",
    "    print(fit_results)\n",
    "\n",
    "    # Extract p-values\n",
    "    p_values = fit_results.pvalues\n",
    "\n",
    "    # Count the number of features that has p-value > 0.05\n",
    "    count_greater_than_005 = np.sum(p_values > 0.05)\n",
    "\n",
    "    # Evaluation\n",
    "    MSE = fit_results.mse_total\n",
    "    print(f\"Number of attributes that are not significant: {count_greater_than_005} / {len(X_train.columns)}\")\n",
    "    print(f\"MSE: {MSE}\")\n",
    "\n",
    "    return fit_results\n",
    "\n",
    "linear_regression_trained_model = linear_regression_model(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of values < 0.5: 3304\n",
      "Number of values >= 0.5: 162\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def predict_test_data(linear_regression_trained_model, X_test, y_test):\n",
    "    X_test_with_constant = sm.add_constant(X_test)\n",
    "\n",
    "    predictions = linear_regression_trained_model.predict(X_test_with_constant)\n",
    "    return predictions\n",
    "\n",
    "outcome = predict_test_data(linear_regression_trained_model, X_test, y_test)\n",
    "# Count the number of values less than 0.5\n",
    "count_less_than_05 = np.sum(outcome < 0.5)\n",
    "\n",
    "# Count the number of values greater than or equal to 0.5\n",
    "count_greater_than_or_equal_05 = np.sum(outcome >= 0.5)\n",
    "discrete_outcome = outcome.apply(lambda x: 0 if x < 0.05 else x)\n",
    "\n",
    "print(\"Number of values < 0.5:\", count_less_than_05)\n",
    "print(\"Number of values >= 0.5:\", count_greater_than_or_equal_05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.953260242354299\n"
     ]
    }
   ],
   "source": [
    "def accuracy(y_true, y_pred, tolerance=0):\n",
    "    \"\"\"\n",
    "    Calculate the accuracy of predictions within a tolerance range.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_true: Array-like, true target values.\n",
    "    - y_pred: Array-like, predicted target values.\n",
    "    - tolerance: float, the tolerance range around the true values.\n",
    "    \n",
    "    Returns:\n",
    "    - acc: float, the accuracy of predictions within the tolerance range.\n",
    "    \"\"\"\n",
    "    # Calculate the absolute errors\n",
    "    errors = abs(y_pred - y_true)\n",
    "    \n",
    "    # Count the number of predictions within the tolerance range\n",
    "    within_tolerance = sum(errors <= tolerance)\n",
    "    \n",
    "    # Calculate the total number of predictions\n",
    "    total_predictions = len(y_true)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    acc = within_tolerance / total_predictions\n",
    "    \n",
    "    return acc\n",
    "\n",
    "# Calculate accuracy using the function\n",
    "acc = accuracy(y_test.values.flatten(), discrete_outcome)\n",
    "\n",
    "print(\"Accuracy:\", acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: inf\n",
      "         Iterations: 35\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [253], line 19\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(fit_results\u001b[38;5;241m.\u001b[39msummary())\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_results\n\u001b[0;32m---> 19\u001b[0m logistic_trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mlogistic_regression_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [253], line 11\u001b[0m, in \u001b[0;36mlogistic_regression_model\u001b[0;34m(X_train, y_train)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Fit logistic regression model\u001b[39;00m\n\u001b[1;32m     10\u001b[0m logit_model \u001b[38;5;241m=\u001b[39m sm\u001b[38;5;241m.\u001b[39mLogit(y_train, X_train_with_constant)\n\u001b[0;32m---> 11\u001b[0m fit_results \u001b[38;5;241m=\u001b[39m \u001b[43mlogit_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Print summary of the model\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(fit_results\u001b[38;5;241m.\u001b[39msummary())\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/statsmodels/discrete/discrete_model.py:2601\u001b[0m, in \u001b[0;36mLogit.fit\u001b[0;34m(self, start_params, method, maxiter, full_output, disp, callback, **kwargs)\u001b[0m\n\u001b[1;32m   2598\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(DiscreteModel\u001b[38;5;241m.\u001b[39mfit\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m)\n\u001b[1;32m   2599\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, start_params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnewton\u001b[39m\u001b[38;5;124m'\u001b[39m, maxiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m35\u001b[39m,\n\u001b[1;32m   2600\u001b[0m         full_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, disp\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 2601\u001b[0m     bnryfit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2602\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2603\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2604\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mfull_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2605\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mdisp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2606\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2607\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2609\u001b[0m     discretefit \u001b[38;5;241m=\u001b[39m LogitResults(\u001b[38;5;28mself\u001b[39m, bnryfit)\n\u001b[1;32m   2610\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m BinaryResultsWrapper(discretefit)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/statsmodels/discrete/discrete_model.py:243\u001b[0m, in \u001b[0;36mDiscreteModel.fit\u001b[0;34m(self, start_params, method, maxiter, full_output, disp, callback, **kwargs)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# TODO: make a function factory to have multiple call-backs\u001b[39;00m\n\u001b[0;32m--> 243\u001b[0m mlefit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mfull_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mdisp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mlefit\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/statsmodels/base/model.py:582\u001b[0m, in \u001b[0;36mLikelihoodModel.fit\u001b[0;34m(self, start_params, method, maxiter, full_output, disp, fargs, callback, retall, skip_hessian, **kwargs)\u001b[0m\n\u001b[1;32m    580\u001b[0m     Hinv \u001b[38;5;241m=\u001b[39m cov_params_func(\u001b[38;5;28mself\u001b[39m, xopt, retvals)\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnewton\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m full_output:\n\u001b[0;32m--> 582\u001b[0m     Hinv \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mretvals\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHessian\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m nobs\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_hessian:\n\u001b[1;32m    584\u001b[0m     H \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhessian(xopt)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/linalg/linalg.py:561\u001b[0m, in \u001b[0;36minv\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    559\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD->D\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md->d\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    560\u001b[0m extobj \u001b[38;5;241m=\u001b[39m get_linalg_error_extobj(_raise_linalgerror_singular)\n\u001b[0;32m--> 561\u001b[0m ainv \u001b[38;5;241m=\u001b[39m \u001b[43m_umath_linalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap(ainv\u001b[38;5;241m.\u001b[39mastype(result_t, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/linalg/linalg.py:112\u001b[0m, in \u001b[0;36m_raise_linalgerror_singular\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raise_linalgerror_singular\u001b[39m(err, flag):\n\u001b[0;32m--> 112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSingular matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "def logistic_regression_model(X_train, y_train):\n",
    "    # Reset indices to ensure alignment\n",
    "    X_train.reset_index(drop=True, inplace=True)\n",
    "    y_train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Add constant column to the features\n",
    "    X_train_with_constant = sm.add_constant(X_train)\n",
    "\n",
    "    # Fit logistic regression model\n",
    "    logit_model = sm.Logit(y_train, X_train_with_constant)\n",
    "    fit_results = logit_model.fit()\n",
    "\n",
    "    # Print summary of the model\n",
    "    print(fit_results.summary())\n",
    "\n",
    "    return fit_results\n",
    "\n",
    "\n",
    "logistic_trained_model = logistic_regression_model(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
