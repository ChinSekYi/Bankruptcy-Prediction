{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform feature selection to find the best set of features for out Machine learning models.   \n",
    "Because irrelavant features can:\n",
    "- **Increase Model Complexity**: Because necessary features can lead to overfitting, where the model performs well on training data but poorly on unseen data.\n",
    "- **Reduce Model Interpretability**: A large number of features makes it difficult to understand how the model arrives at its predictions.\n",
    "- **Slow Down Training Time**: Training models with irrelevant features takes longer and consumes more computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "- [Essential Data Processing Techniques](#essential-data-processing-techniques)  \n",
    "    - [SMOTE Technique to handle imbalanced class](#smote-technique-to-handle-imbalanced-class)\n",
    "- [Filter Methods](#filter-methods)\n",
    "    - [Correlation coefficient](#correlation-coefficient)  \n",
    "    - [Chi-Square test](#chi-square-test)\n",
    "    - [ANOVA F-test](#anova-f-test)  \n",
    "    - [Information Gain](#information-gain)\n",
    "\n",
    "- [Wrapper Methods](#wrapper-methods)\n",
    "   - [Forward Selection](#forward-feature-selection)\n",
    "   - [Backward Elimination](#backward-feature-elimination)\n",
    "   - [Recursive Feature Elimination](#recursive-feature-elimination)\n",
    "\n",
    "- [Embedded Methods](#embedded-methods)\n",
    "    - [LASSO Regularisation (L1)]()\n",
    "\n",
    "- [Advanced Methods](#advanced-methods)\n",
    "    - [PCA](#pca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import data_cleaning as dc\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from data_cleaning import get_Xy, med_impute, normalise, drop_high_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acess dataframe from EDA step\n",
    "df_to_share = pd.read_pickle(\"df_to_share.pkl\")\n",
    "df_to_share.head()\n",
    "df = df_to_share.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Essential Data Processing techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: remove this code chunk\n",
    "def process0(df):\n",
    "    X, y = get_Xy(df)\n",
    "    X_imputed, y = med_impute(X, y)\n",
    "    X_scaled_df = normalise(X_imputed)\n",
    "    return X_scaled_df, y\n",
    "\n",
    "\n",
    "# function to pre-process the data\n",
    "def process1(df):\n",
    "    X, y = get_Xy(df)\n",
    "    X_imputed, y = med_impute(X, y)\n",
    "    X_scaled_df = normalise(X_imputed)\n",
    "    return drop_high_corr(X_scaled_df), y\n",
    "\n",
    "\n",
    "# function to obtain train and test sets\n",
    "def process2(df):\n",
    "    X, y = process1(df)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=3244\n",
    "    )\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "# function to obtain train and test sets with sythesised instances of the minority class\n",
    "def pre_process(df):\n",
    "    X, y = process1(df)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=3244\n",
    "    )\n",
    "    smote = SMOTE(random_state=3244)\n",
    "    X_smote, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    return X_smote, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE Technique to handle imbalanced class\n",
    "- **Purpose**: Address class imbalance in datasets.\n",
    "- **Avoid Undersampling and Oversampling**: Prevents reduction of dataset size and duplication of data respectively.\n",
    "- **Method**: SMOTE generates synthetic samples for the minority class by interpolating between existing minority samples.\n",
    "- **Advantage**: Balances the dataset without losing valuable information, enhancing model performance on imbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### my functions\n",
    "def preprocess(df):\n",
    "    X, y = get_Xy(df)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=3244\n",
    "    )\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def preprocess_with_SMOTE(df):\n",
    "    X, y = get_Xy(df)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=3244\n",
    "    )\n",
    "    smote = SMOTE(random_state=3244)\n",
    "    X_smote, y_train = smote.fit_resample(X_train, y_train)\n",
    "    return X_smote, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter method is a feature selection method in the preprocessing step, and it is independent of the machine learning algorithm deployed afterwards. In our case, we implemented three filtering methods:\n",
    "\n",
    "1. **Correlation**\n",
    "2. **ANOVA F-test**\n",
    "3. **Information Gain (IG)**\n",
    "\n",
    "For both functions, we sort the features based on their scores in descending order. Higher scores indicate more important features.\n",
    "\n",
    "For better judging the optimal number of feature, we define a function `plot_feature_performance` to evaluate the performance of a model (balanced Random Forest as example) as we vary the number of top features used. Finally, it plots the performance metrics f1 and recall against the number of features. With the aid of the plot, we determine the point of diminishing return through elbow method, selecting the optimal number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Coefficient\n",
    "If two features are correlated, we can predict one feature from the other. Hence, we can drop one of the feature, as the second one does not add additional information.  \n",
    "\n",
    "Here, we set the correlation threshold for highly correlated features as `0.7`. If two features are highly correlated, we can drop the feature with a lower correlation coefficient value to the target variable.  However, this is not applicable in our case since our target variable is a categorical variable.   \n",
    "\n",
    "We can also check for multicollinearity for correlation between more than 2 features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula for Pearson's correlation coefficient between two variables $X$ and $Y$ with $n$ observations is calculated as:\n",
    "\n",
    "$$ r = \\frac{\\sum_{i=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sqrt{\\sum_{i=1}^{n} (X_i - \\bar{X})^2} \\sqrt{\\sum_{i=1}^{n} (Y_i - \\bar{Y})^2}} $$\n",
    "\n",
    "where:\n",
    "- $X_{i}$ and $Y_{i}$ are the individual observations of variables $X$ and $Y$, respectively.\n",
    "- $\\bar{X}$ and $\\bar{Y}$ are the means of variables $X$ and $Y$, respectively.\n",
    "\n",
    "This formula computes the normalized covariance between $X$ and $Y$, indicating the strength and direction of their linear relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot convert numpy.ndarray to numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m df_graph \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m----> 2\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_graph\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m, in \u001b[0;36mpreprocess\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess\u001b[39m(df):\n\u001b[0;32m----> 3\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[43mget_Xy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m      5\u001b[0m         X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3244\u001b[39m\n\u001b[1;32m      6\u001b[0m     )\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X_train, X_test, y_train, y_test\n",
      "File \u001b[0;32m~/Downloads/Personal_Projects/Bankruptcy-Prediction/notebook/data_cleaning.py:55\u001b[0m, in \u001b[0;36mget_Xy\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_Xy\u001b[39m(df):\n\u001b[0;32m---> 55\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     56\u001b[0m     y \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.11/site-packages/pandas/core/indexing.py:1184\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_value(\u001b[38;5;241m*\u001b[39mkey, takeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_takeable)\n\u001b[0;32m-> 1184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1186\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.11/site-packages/pandas/core/indexing.py:1694\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1691\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m suppress(IndexingError):\n\u001b[1;32m   1692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_lowerdim(tup)\n\u001b[0;32m-> 1694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_tuple_same_dim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.11/site-packages/pandas/core/indexing.py:1020\u001b[0m, in \u001b[0;36m_LocationIndexer._getitem_tuple_same_dim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m com\u001b[38;5;241m.\u001b[39mis_null_slice(key):\n\u001b[1;32m   1018\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m-> 1020\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mretval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;66;03m# We should never have retval.ndim < self.ndim, as that should\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;66;03m#  be handled by the _getitem_lowerdim call above.\u001b[39;00m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m retval\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.11/site-packages/pandas/core/indexing.py:1729\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1723\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\n\u001b[1;32m   1724\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame indexer is not allowed for .iloc\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1725\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConsider using .loc for automatic alignment.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1726\u001b[0m     )\n\u001b[1;32m   1728\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m):\n\u001b[0;32m-> 1729\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_slice_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   1732\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.11/site-packages/pandas/core/indexing.py:1765\u001b[0m, in \u001b[0;36m_iLocIndexer._get_slice_axis\u001b[0;34m(self, slice_obj, axis)\u001b[0m\n\u001b[1;32m   1763\u001b[0m labels \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[1;32m   1764\u001b[0m labels\u001b[38;5;241m.\u001b[39m_validate_positional_slice(slice_obj)\n\u001b[0;32m-> 1765\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslice_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.11/site-packages/pandas/core/generic.py:4369\u001b[0m, in \u001b[0;36mNDFrame._slice\u001b[0;34m(self, slobj, axis)\u001b[0m\n\u001b[1;32m   4367\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(slobj, \u001b[38;5;28mslice\u001b[39m), \u001b[38;5;28mtype\u001b[39m(slobj)\n\u001b[1;32m   4368\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_block_manager_axis(axis)\n\u001b[0;32m-> 4369\u001b[0m new_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4370\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_mgr, axes\u001b[38;5;241m=\u001b[39mnew_mgr\u001b[38;5;241m.\u001b[39maxes)\n\u001b[1;32m   4371\u001b[0m result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32minternals.pyx:869\u001b[0m, in \u001b[0;36mpandas._libs.internals.BlockManager.get_slice\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.11/site-packages/pandas/core/internals/managers.py:782\u001b[0m, in \u001b[0;36mBaseBlockManager._slice_take_blocks_ax0\u001b[0;34m(self, slice_or_indexer, fill_value, only_slice, use_na_proxy, ref_inplace_op)\u001b[0m\n\u001b[1;32m    772\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    773\u001b[0m                 blk\u001b[38;5;241m.\u001b[39mtake_nd(\n\u001b[1;32m    774\u001b[0m                     slobj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    778\u001b[0m                 )\n\u001b[1;32m    779\u001b[0m             ]\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sl_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mslice\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 782\u001b[0m     blknos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblknos\u001b[49m[slobj]\n\u001b[1;32m    783\u001b[0m     blklocs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblklocs[slobj]\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.11/site-packages/pandas/core/internals/managers.py:192\u001b[0m, in \u001b[0;36mBaseBlockManager.blknos\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03mSuppose we want to find the array corresponding to our i'th column.\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;124;03mself.blocks[self.blknos[i]]\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blknos \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;66;03m# Note: these can be altered by other BlockManager methods.\u001b[39;00m\n\u001b[0;32m--> 192\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rebuild_blknos_and_blklocs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blknos\n",
      "File \u001b[0;32minternals.pyx:761\u001b[0m, in \u001b[0;36mpandas._libs.internals.BlockManager._rebuild_blknos_and_blklocs\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot convert numpy.ndarray to numpy.ndarray"
     ]
    }
   ],
   "source": [
    "df_graph = df.copy()\n",
    "X_train, X_test, y_train, y_test = preprocess(df_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for all predictive features in the training dataset\n",
    "cor = X_train.corr()\n",
    "cor.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(X_train.corr(), annot=False, cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_cleaning import drop_high_corr, drop_corr_columns_from_test\n",
    "\n",
    "print(\"Before dropping high correlated features: \", X_train.shape)\n",
    "\n",
    "X_train_dropped, dropped_features = drop_high_corr(X_train, threshold=0.7)\n",
    "X_test_dropped = drop_corr_columns_from_test(X_test, dropped_features)\n",
    "\n",
    "print(\"After dropping high correlated features: \", X_train_dropped.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation:  \n",
    "- 34 features are dropped "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi-square test\n",
    "Measures the independence between categorical features and the target variable.\n",
    "Since we do not have independent features in our dataset, we shall not use Chi-Square test for feature selection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANOVA F-test\n",
    "**ANOVA (Analysis of Variance)** is a statistical test used in feature selection to identify features that have a significant influence on the target variable. Used for linear models only.\n",
    "\n",
    "Steps to select the best k features (where k is a chosen number):\n",
    "- Focuses on Variance: \n",
    "    - ANOVA analyses the variation in a feature's values. It compares the variation between groups (bankrupt vs. non-bankrupt) to the variation within each group.\n",
    "- Low p-value indicates Impact: (Reject H0)\n",
    "    - Features with a low p-value (from the F-statistic) suggest a strong statistical difference in the feature's values between bankrupt and non-bankrupt companies. This implies the feature likely has a real impact on predicting bankruptcy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ANOVA_test_graph(train_acc_dict, test_acc_dict):\n",
    "    # Extract keys and values from train_acc_dict and test_acc_dict\n",
    "    train_k_values, train_accuracy_values = zip(*train_acc_dict.items())\n",
    "    test_k_values, test_accuracy_values = zip(*test_acc_dict.items())\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    # Plot train accuracy\n",
    "    plt.plot(\n",
    "        train_k_values, train_accuracy_values, label=\"Train Accuracy\", color=\"blue\"\n",
    "    )\n",
    "    # Plot test accuracy\n",
    "    plt.plot(test_k_values, test_accuracy_values, label=\"Test Accuracy\", color=\"green\")\n",
    "\n",
    "    # Find k values corresponding to maximum accuracies\n",
    "    best_train_k = max(train_acc_dict, key=train_acc_dict.get)\n",
    "    best_test_k = max(test_acc_dict, key=test_acc_dict.get)\n",
    "    best_train_accuracy = train_acc_dict[best_train_k]\n",
    "    best_test_accuracy = test_acc_dict[best_test_k]\n",
    "\n",
    "    # Annotate the point corresponding to the peak train accuracy\n",
    "    plt.annotate(\n",
    "        f\"Max Train Accuracy\\nk={best_train_k}, Acc={best_train_accuracy:.2f}\",\n",
    "        xy=(best_train_k, best_train_accuracy),\n",
    "        xytext=(-30, 20),\n",
    "        textcoords=\"offset points\",\n",
    "        arrowprops=dict(arrowstyle=\"->\", color=\"blue\"),\n",
    "    )\n",
    "\n",
    "    # Annotate the point corresponding to the peak test accuracy\n",
    "    plt.annotate(\n",
    "        f\"Max Test Accuracy\\nk={best_test_k}, Acc={best_test_accuracy:.2f}\",\n",
    "        xy=(best_test_k, best_test_accuracy),\n",
    "        xytext=(30, -30),\n",
    "        textcoords=\"offset points\",\n",
    "        arrowprops=dict(arrowstyle=\"->\", color=\"green\"),\n",
    "    )\n",
    "\n",
    "    # Label axes and add title\n",
    "    plt.xlabel(\"Number of Features (k)\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Accuracy vs. Number of Features from ANOVA test\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "## Note: *args follow the convention X_train, X_test, y_train, y_test\n",
    "def get_df_with_top_k_features(k_features, *args):  # after pre_process(df)\n",
    "    X_train = args[0]\n",
    "    X_test = args[1]\n",
    "    y_train = args[2]\n",
    "    y_test = args[3]\n",
    "\n",
    "    # define feature selection\n",
    "    fs = SelectKBest(score_func=f_classif, k=k_features)\n",
    "\n",
    "    # apply feature selection\n",
    "    fs.fit_transform(X_train, y_train)\n",
    "\n",
    "    # Take the features with the highest F-scores\n",
    "    fs_scores_array = np.array(fs.scores_)\n",
    "\n",
    "    # Get the indices that would sort the array in descending order\n",
    "    sorted_indices_desc = np.argsort(fs_scores_array)[::-1]\n",
    "\n",
    "    # Take the top k indices\n",
    "    top_indices = sorted_indices_desc[:k_features]\n",
    "\n",
    "    selected_columns_X_train = X_train.iloc[:, top_indices]\n",
    "    selected_columns_X_test = X_test.iloc[:, top_indices]\n",
    "\n",
    "    return selected_columns_X_train, selected_columns_X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_k_features_from_ANOVA(model, *args):\n",
    "    X_train = args[0]\n",
    "    original_n_features = len(X_train.columns)\n",
    "\n",
    "    train_acc_dict = {}  # 0 is a dummy accuracy for k=0 features\n",
    "    test_acc_dict = {}\n",
    "    train_test_dataset = {}\n",
    "\n",
    "    for k in range(1, original_n_features + 1):\n",
    "        train_test_dataset_after_ANOVA = get_df_with_top_k_features(k, *args)\n",
    "        train_accuracy, test_accuracy = model(*train_test_dataset_after_ANOVA)\n",
    "        train_test_dataset[k] = train_test_dataset_after_ANOVA\n",
    "        train_acc_dict[k] = train_accuracy\n",
    "        test_acc_dict[k] = test_accuracy\n",
    "        print(\n",
    "            f\"k: {k}, train_accuracy: {train_accuracy}, test_accuracy: {test_accuracy}\"\n",
    "        )\n",
    "\n",
    "    # Find k that gives the highest accuracy\n",
    "    best_train_k = max(train_acc_dict, key=train_acc_dict.get)\n",
    "    best_test_k = max(test_acc_dict, key=test_acc_dict.get)\n",
    "\n",
    "    print(f\"\\033[96mBest k for train_accuracy:\\033[00m {best_train_k}\")\n",
    "    print(f\"\\033[96mBest k for test_accuracy:\\033[00m {best_test_k}\")\n",
    "\n",
    "    plot_ANOVA_test_graph(train_acc_dict, test_acc_dict)\n",
    "\n",
    "    return train_test_dataset[best_test_k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using ANOVA test for feature selection on our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "def train_and_evaluate_svm(*args):\n",
    "    params = {\"kernel\": \"sigmoid\"}\n",
    "    svm_model = SVC(**params)\n",
    "    train_accuracy, test_accuracy = dc.train_and_evaluate_model(svm_model, *args)\n",
    "    return train_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: remove this code chuck\n",
    "train_test_dataset = preprocess(df)\n",
    "train_test_smote_dataset = preprocess_with_SMOTE(df)\n",
    "\n",
    "svm_param = {\"kernel\": \"sigmoid\"}\n",
    "svm_model_result = train_and_evaluate_svm(*train_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_dataset = preprocess(df)\n",
    "train_test_smote_dataset = preprocess_with_SMOTE(df)\n",
    "\n",
    "# Example of using the function\n",
    "# best_train_test_df = find_best_k_features_from_ANOVA(train_and_evaluate_svm, *train_test_dataset)\n",
    "\n",
    "# Since we have class imbalance, we will use the dataset with SMOTE\n",
    "best_train_test_smote_df = find_best_k_features_from_ANOVA(\n",
    "    train_and_evaluate_svm, *train_test_smote_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation:  \n",
    "Using the ANOVA test feature selection method, we see that using ANOVA F-test to choose the best k features for different k values do not seem to affect the accuracy of the model, since the accuracy lays between 0.915 to 0.95 for k values 1-64.\n",
    "\n",
    "Hence, using ANOVA test feature selection is not suitable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measures the reduction in entropy from transforming a dataset in some way. It is often used in training decision trees. Information Gain can be applied to feature selection by evaluating the mutual information between each feature and the target variable. Features that have higher mutual information with the target variable are considered more informative and are thus selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "def information_gain(X, y):\n",
    "    mi_scores = mutual_info_classif(X, y, discrete_features=\"auto\")\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n",
    "\n",
    "\n",
    "def correlation_selection(X, y):\n",
    "    if isinstance(y, pd.Series):\n",
    "        y = y.to_frame(name=\"Target\")\n",
    "    elif isinstance(y, pd.DataFrame):\n",
    "        y.columns = [\"Target\"]\n",
    "\n",
    "    df_combined = pd.concat([X, y], axis=1)\n",
    "    correlation_matrix = df_combined.corr()\n",
    "    correlation_w_target = correlation_matrix[\"Target\"].drop(\"Target\")\n",
    "    return correlation_w_target.abs().sort_values(ascending=False)\n",
    "\n",
    "\n",
    "def plot_feature_performance(X, y, score_series, model, max_features=None):\n",
    "    f1_results = []\n",
    "    recall_results = []\n",
    "    feature_counts = []\n",
    "\n",
    "    if not max_features:\n",
    "        max_features = len(score_series)\n",
    "\n",
    "    for i in range(1, max_features + 1):\n",
    "        top_features = score_series.nlargest(i).index\n",
    "        X_selected = X[top_features]\n",
    "\n",
    "        # Cross-validation F1 and recall scores\n",
    "        f1 = cross_val_score(model, X_selected, y, cv=5, scoring=\"f1\")\n",
    "        recall = cross_val_score(model, X_selected, y, cv=5, scoring=\"recall\")\n",
    "\n",
    "        f1_results.append(f1.mean())\n",
    "        recall_results.append(recall.mean())\n",
    "        feature_counts.append(i)\n",
    "\n",
    "    # Plotting the F1-score and recall results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(\n",
    "        feature_counts,\n",
    "        f1_results,\n",
    "        marker=\"o\",\n",
    "        linestyle=\"-\",\n",
    "        markersize=8,\n",
    "        label=\"F1 Score\",\n",
    "    )\n",
    "    plt.plot(\n",
    "        feature_counts,\n",
    "        recall_results,\n",
    "        marker=\"o\",\n",
    "        linestyle=\"-\",\n",
    "        markersize=8,\n",
    "        label=\"Recall\",\n",
    "    )\n",
    "    plt.xlabel(\"Number of Features\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(\"Model Performance vs. Number of Features\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using the function\n",
    "train_test_dataset = preprocess(df)\n",
    "train_test_smote_dataset = preprocess_with_SMOTE(df)\n",
    "\n",
    "svm_model = SVC(kernel=\"sigmoid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper Methods\n",
    "Wrappers require some method to search the space of all possible subsets of features, assessing their quality by learning and evaluating a classifier with that feature subset. The feature selection process is based on a specific machine learning algorithm we are trying to fit on a given dataset. It follows a greedy search approach by evaluating all the possible combinations of features against the evaluation criterion. The wrapper methods usually result in better predictive accuracy than filter methods.\n",
    "\n",
    "Wrapper methods:\n",
    "1. Forward Feature Selection\n",
    "2. Backward Feature Elimination\n",
    "3. Exhaustive Feature Selection\n",
    "4. Recursive Feature Elimination\n",
    "\n",
    "The wrapper method for feature selection is a technique used to identify the most significant features for a predictive model. It works by starting with an empty set and adding features one by one, each time choosing the feature that, when added, most improves the model's F1 score - our desirable metric. This approach is iterative and selects features based on their contribution to the model's predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "def forward_feature_selection(model, X_train, y_train, X_test, y_test):\n",
    "    selected_features = []\n",
    "    best_f1 = 0\n",
    "    features = list(X_train.columns)\n",
    "\n",
    "    for _ in range(len(features)):\n",
    "        f1_scores = []\n",
    "        for feature in features:\n",
    "            if feature not in selected_features:\n",
    "                temp_features = selected_features + [feature]\n",
    "                model.fit(X_train[temp_features], y_train)\n",
    "                y_pred = model.predict(X_test[temp_features])\n",
    "                f1 = f1_score(y_test, y_pred)\n",
    "                f1_scores.append((feature, f1))\n",
    "\n",
    "        # Find the best feature and its f1 score\n",
    "        f1_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        best_feature, best_feature_f1 = f1_scores[0]\n",
    "\n",
    "        if best_feature_f1 > best_f1:\n",
    "            print(f\"Adding {best_feature} improved F1 to {best_feature_f1}\")\n",
    "            best_f1 = best_feature_f1\n",
    "            selected_features.append(best_feature)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    print(\"Selected features:\", selected_features)\n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using the function\n",
    "train_test_dataset = preprocess(df)\n",
    "train_test_smote_dataset = preprocess_with_SMOTE(df)\n",
    "\n",
    "svm_model = SVC(kernel=\"sigmoid\")\n",
    "forward_feature_selection(svm_model, train_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO future work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO future work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedded Methods\n",
    "These methods encompass the benefits of both the wrapper and filter methods by including interactions of features but also maintaining reasonable computational costs. Embedded methods are iterative in the sense that takes care of each iteration of the model training process and carefully extract those features which contribute the most to the training for a particular iteration.\n",
    "\n",
    "Example techniques:\n",
    "1. LASSO Regularisation (L1)\n",
    "2. Random Forest Importance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Methods\n",
    "1. Principal Component Analysis (PCA): \n",
    "    - Reduces the dimensionality of the data by transforming it into a new set of features that are linear combinations of the original features.\n",
    "2. Independent Component Analysis (ICA): \n",
    "    - Similar to PCA, but focuses on making the components statistically independent rather than uncorrelated.\n",
    "3. Factor Analysis: \n",
    "    - Identifies underlying relationships between features by modeling the observed variables as linear combinations of potential factors.\n",
    "4. t-Distributed Stochastic Neighbor Embedding (t-SNE): \n",
    "    - Primarily used for visualization but can help in understanding the structure of the data for feature selection.\n",
    "5. Autoencoders: \n",
    "    - Uses neural networks to learn a compressed representation of the data, which can be used for feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO future work"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
