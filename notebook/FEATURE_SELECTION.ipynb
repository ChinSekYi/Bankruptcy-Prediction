{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform feature selection to find the best set of features for out Machine learning models.   \n",
    "Because irrelavant features can:\n",
    "- **Increase Model Complexity**: Because necessary features can lead to overfitting, where the model performs well on training data but poorly on unseen data.\n",
    "- **Reduce Model Interpretability**: A large number of features makes it difficult to understand how the model arrives at its predictions.\n",
    "- **Slow Down Training Time**: Training models with irrelevant features takes longer and consumes more computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "- [Essential Data Processing Techniques](#essential-data-processing-techniques)  \n",
    "    - [SMOTE Technique to handle imbalanced class](#smote-technique-to-handle-imbalanced-class)\n",
    "- [Filter Methods](#filter-methods)\n",
    "    - [Correlation coefficient](#correlation-coefficient)  \n",
    "    - [Chi-Square test](#chi-square-test)\n",
    "    - [ANOVA F-test](#anova-f-test)  \n",
    "    - [Information Gain](#information-gain)\n",
    "\n",
    "- [Wrapper Methods](#wrapper-methods)\n",
    "   - [Forward Selection](#forward-feature-selection)\n",
    "   - [Backward Elimination](#backward-feature-elimination)\n",
    "   - [Recursive Feature Elimination](#recursive-feature-elimination)\n",
    "\n",
    "- [Embedded Methods](#embedded-methods)\n",
    "    - [LASSO Regularisation (L1)]()\n",
    "\n",
    "- [Advanced Methods](#advanced-methods)\n",
    "    - [PCA](#pca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/sekyichin/miniconda3/envs/dev/lib/python3.11/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/sekyichin/miniconda3/envs/dev/lib/python3.11/site-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/sekyichin/miniconda3/envs/dev/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/sekyichin/miniconda3/envs/dev/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Users/sekyichin/miniconda3/envs/dev/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Users/sekyichin/miniconda3/envs/dev/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Users/sekyichin/miniconda3/envs/dev/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/sekyichin/miniconda3/envs/dev/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/sekyichin/miniconda3/envs/dev/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/sekyichin/miniconda3/envs/dev/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/sekyichin/miniconda3/envs/dev/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/sekyichin/miniconda3/envs/dev/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/sekyichin/miniconda3/envs/dev/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/sekyichin/miniconda3/envs/dev/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3051, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/sekyichin/miniconda3/envs/dev/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3106, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/sekyichin/miniconda3/envs/dev/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/sekyichin/miniconda3/envs/dev/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3311, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/sekyichin/miniconda3/envs/dev/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3493, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/sekyichin/miniconda3/envs/dev/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/n3/j78ghkxd2zd7pk3ddt_4cht00000gn/T/ipykernel_22182/879408987.py\", line 3, in <module>\n",
      "    import matplotlib.pyplot as plt\n",
      "  File \"/Users/sekyichin/miniconda3/envs/dev/lib/python3.11/site-packages/matplotlib/__init__.py\", line 161, in <module>\n",
      "    from . import _api, _version, cbook, _docstring, rcsetup\n",
      "  File \"/Users/sekyichin/miniconda3/envs/dev/lib/python3.11/site-packages/matplotlib/rcsetup.py\", line 27, in <module>\n",
      "    from matplotlib.colors import Colormap, is_color_like\n",
      "  File \"/Users/sekyichin/miniconda3/envs/dev/lib/python3.11/site-packages/matplotlib/colors.py\", line 57, in <module>\n",
      "    from matplotlib import _api, _cm, cbook, scale\n",
      "  File \"/Users/sekyichin/miniconda3/envs/dev/lib/python3.11/site-packages/matplotlib/scale.py\", line 22, in <module>\n",
      "    from matplotlib.ticker import (\n",
      "  File \"/Users/sekyichin/miniconda3/envs/dev/lib/python3.11/site-packages/matplotlib/ticker.py\", line 143, in <module>\n",
      "    from matplotlib import transforms as mtransforms\n",
      "  File \"/Users/sekyichin/miniconda3/envs/dev/lib/python3.11/site-packages/matplotlib/transforms.py\", line 49, in <module>\n",
      "    from matplotlib._path import (\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdata_cleaning\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdc\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.11/site-packages/matplotlib/__init__.py:161\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse \u001b[38;5;28;01mas\u001b[39;00m parse_version\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# cbook must import matplotlib only within function\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# definitions, so it is safe to import from it here.\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _version, cbook, _docstring, rcsetup\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sanitize_sequence\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MatplotlibDeprecationWarning\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.11/site-packages/matplotlib/rcsetup.py:27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, cbook\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ls_mapper\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Colormap, is_color_like\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_fontconfig_pattern\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_fontconfig_pattern\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_enums\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JoinStyle, CapStyle\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.11/site-packages/matplotlib/colors.py:57\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _cm, cbook, scale\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_color_data\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BASE_COLORS, TABLEAU_COLORS, CSS4_COLORS, XKCD_COLORS\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01m_ColorMapping\u001b[39;00m(\u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.11/site-packages/matplotlib/scale.py:22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _docstring\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mticker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     23\u001b[0m     NullFormatter, ScalarFormatter, LogFormatterSciNotation, LogitFormatter,\n\u001b[1;32m     24\u001b[0m     NullLocator, LogLocator, AutoLocator, AutoMinorLocator,\n\u001b[1;32m     25\u001b[0m     SymmetricalLogLocator, AsinhLocator, LogitLocator)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Transform, IdentityTransform\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mScaleBase\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.11/site-packages/matplotlib/ticker.py:143\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, cbook\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms \u001b[38;5;28;01mas\u001b[39;00m mtransforms\n\u001b[1;32m    145\u001b[0m _log \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    147\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTickHelper\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFixedFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    148\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNullFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFuncFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFormatStrFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    149\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStrMethodFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScalarFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    155\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMultipleLocator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMaxNLocator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAutoMinorLocator\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    156\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSymmetricalLogLocator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAsinhLocator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogitLocator\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.11/site-packages/matplotlib/transforms.py:49\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m inv\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_path\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     50\u001b[0m     affine_transform, count_bboxes_overlapping_bbox, update_path_extents)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m     53\u001b[0m DEBUG \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import data_cleaning as dc\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from data_cleaning import get_Xy, med_impute, normalise, drop_high_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acess dataframe from EDA step\n",
    "df_to_share = pd.read_pickle(\"df_to_share.pkl\")\n",
    "df_to_share.head()\n",
    "df = df_to_share.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Essential Data Processing techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: remove this code chunk\n",
    "def process0(df):\n",
    "    X, y = get_Xy(df)\n",
    "    X_imputed, y = med_impute(X, y)\n",
    "    X_scaled_df = normalise(X_imputed)\n",
    "    return X_scaled_df, y\n",
    "\n",
    "\n",
    "# function to pre-process the data\n",
    "def process1(df):\n",
    "    X, y = get_Xy(df)\n",
    "    X_imputed, y = med_impute(X, y)\n",
    "    X_scaled_df = normalise(X_imputed)\n",
    "    return drop_high_corr(X_scaled_df), y\n",
    "\n",
    "\n",
    "# function to obtain train and test sets\n",
    "def process2(df):\n",
    "    X, y = process1(df)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=3244\n",
    "    )\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "# function to obtain train and test sets with sythesised instances of the minority class\n",
    "def pre_process(df):\n",
    "    X, y = process1(df)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=3244\n",
    "    )\n",
    "    smote = SMOTE(random_state=3244)\n",
    "    X_smote, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    return X_smote, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE Technique to handle imbalanced class\n",
    "- **Purpose**: Address class imbalance in datasets.\n",
    "- **Avoid Undersampling and Oversampling**: Prevents reduction of dataset size and duplication of data respectively.\n",
    "- **Method**: SMOTE generates synthetic samples for the minority class by interpolating between existing minority samples.\n",
    "- **Advantage**: Balances the dataset without losing valuable information, enhancing model performance on imbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### my functions\n",
    "def preprocess(df):\n",
    "    X, y = get_Xy(df)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=3244\n",
    "    )\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def preprocess_with_SMOTE(df):\n",
    "    X, y = get_Xy(df)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=3244\n",
    "    )\n",
    "    smote = SMOTE(random_state=3244)\n",
    "    X_smote, y_train = smote.fit_resample(X_train, y_train)\n",
    "    return X_smote, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter method is a feature selection method in the preprocessing step, and it is independent of the machine learning algorithm deployed afterwards. In our case, we implemented three filtering methods:\n",
    "\n",
    "1. **Correlation**\n",
    "2. **ANOVA F-test**\n",
    "3. **Information Gain (IG)**\n",
    "\n",
    "For both functions, we sort the features based on their scores in descending order. Higher scores indicate more important features.\n",
    "\n",
    "For better judging the optimal number of feature, we define a function `plot_feature_performance` to evaluate the performance of a model (balanced Random Forest as example) as we vary the number of top features used. Finally, it plots the performance metrics f1 and recall against the number of features. With the aid of the plot, we determine the point of diminishing return through elbow method, selecting the optimal number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Coefficient\n",
    "If two features are correlated, we can predict one feature from the other. Hence, we can drop one of the feature, as the second one does not add additional information.  \n",
    "\n",
    "Here, we set the correlation threshold for highly correlated features as `0.7`. If two features are highly correlated, we can drop the feature with a lower correlation coefficient value to the target variable.  However, this is not applicable in our case since our target variable is a categorical variable.   \n",
    "\n",
    "We can also check for multicollinearity for correlation between more than 2 features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula for Pearson's correlation coefficient between two variables $X$ and $Y$ with $n$ observations is calculated as:\n",
    "\n",
    "$$ r = \\frac{\\sum_{i=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sqrt{\\sum_{i=1}^{n} (X_i - \\bar{X})^2} \\sqrt{\\sum_{i=1}^{n} (Y_i - \\bar{Y})^2}} $$\n",
    "\n",
    "where:\n",
    "- $X_{i}$ and $Y_{i}$ are the individual observations of variables $X$ and $Y$, respectively.\n",
    "- $\\bar{X}$ and $\\bar{Y}$ are the means of variables $X$ and $Y$, respectively.\n",
    "\n",
    "This formula computes the normalized covariance between $X$ and $Y$, indicating the strength and direction of their linear relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph = df.copy()\n",
    "X_train, X_test, y_train, y_test = preprocess(df_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for all predictive features in the training dataset\n",
    "cor = X_train.corr()\n",
    "cor.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(X_train.corr(), annot=False, cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_cleaning import drop_high_corr, drop_corr_columns_from_test\n",
    "\n",
    "print(\"Before dropping high correlated features: \", X_train.shape)\n",
    "\n",
    "X_train_dropped, dropped_features = drop_high_corr(X_train, threshold=0.7)\n",
    "X_test_dropped = drop_corr_columns_from_test(X_test, dropped_features)\n",
    "\n",
    "print(\"After dropping high correlated features: \", X_train_dropped.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation:  \n",
    "- 34 features are dropped "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi-square test\n",
    "Measures the independence between categorical features and the target variable.\n",
    "Since we do not have independent features in our dataset, we shall not use Chi-Square test for feature selection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANOVA F-test\n",
    "**ANOVA (Analysis of Variance)** is a statistical test used in feature selection to identify features that have a significant influence on the target variable. \n",
    "\n",
    "Steps to select the best k features (where k is a chosen number):\n",
    "- Focuses on Variance: \n",
    "    - ANOVA analyses the variation in a feature's values. It compares the variation between groups (bankrupt vs. non-bankrupt) to the variation within each group.\n",
    "- Low p-value indicates Impact: \n",
    "    - Features with a low p-value (from the F-statistic) suggest a strong statistical difference in the feature's values between bankrupt and non-bankrupt companies. This implies the feature likely has a real impact on predicting bankruptcy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ANOVA_test_graph(train_acc_dict, test_acc_dict):\n",
    "    # Extract keys and values from train_acc_dict and test_acc_dict\n",
    "    train_k_values, train_accuracy_values = zip(*train_acc_dict.items())\n",
    "    test_k_values, test_accuracy_values = zip(*test_acc_dict.items())\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    # Plot train accuracy\n",
    "    plt.plot(\n",
    "        train_k_values, train_accuracy_values, label=\"Train Accuracy\", color=\"blue\"\n",
    "    )\n",
    "    # Plot test accuracy\n",
    "    plt.plot(test_k_values, test_accuracy_values, label=\"Test Accuracy\", color=\"green\")\n",
    "\n",
    "    # Find k values corresponding to maximum accuracies\n",
    "    best_train_k = max(train_acc_dict, key=train_acc_dict.get)\n",
    "    best_test_k = max(test_acc_dict, key=test_acc_dict.get)\n",
    "    best_train_accuracy = train_acc_dict[best_train_k]\n",
    "    best_test_accuracy = test_acc_dict[best_test_k]\n",
    "\n",
    "    # Annotate the point corresponding to the peak train accuracy\n",
    "    plt.annotate(\n",
    "        f\"Max Train Accuracy\\nk={best_train_k}, Acc={best_train_accuracy:.2f}\",\n",
    "        xy=(best_train_k, best_train_accuracy),\n",
    "        xytext=(-30, 20),\n",
    "        textcoords=\"offset points\",\n",
    "        arrowprops=dict(arrowstyle=\"->\", color=\"blue\"),\n",
    "    )\n",
    "\n",
    "    # Annotate the point corresponding to the peak test accuracy\n",
    "    plt.annotate(\n",
    "        f\"Max Test Accuracy\\nk={best_test_k}, Acc={best_test_accuracy:.2f}\",\n",
    "        xy=(best_test_k, best_test_accuracy),\n",
    "        xytext=(30, -30),\n",
    "        textcoords=\"offset points\",\n",
    "        arrowprops=dict(arrowstyle=\"->\", color=\"green\"),\n",
    "    )\n",
    "\n",
    "    # Label axes and add title\n",
    "    plt.xlabel(\"Number of Features (k)\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Accuracy vs. Number of Features from ANOVA test\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "## Note: *args follow the convention X_train, X_test, y_train, y_test\n",
    "def get_df_with_top_k_features(k_features, *args):  # after pre_process(df)\n",
    "    X_train = args[0]\n",
    "    X_test = args[1]\n",
    "    y_train = args[2]\n",
    "    y_test = args[3]\n",
    "\n",
    "    # define feature selection\n",
    "    fs = SelectKBest(score_func=f_classif, k=k_features)\n",
    "\n",
    "    # apply feature selection\n",
    "    fs.fit_transform(X_train, y_train)\n",
    "\n",
    "    # Take the features with the highest F-scores\n",
    "    fs_scores_array = np.array(fs.scores_)\n",
    "\n",
    "    # Get the indices that would sort the array in descending order\n",
    "    sorted_indices_desc = np.argsort(fs_scores_array)[::-1]\n",
    "\n",
    "    # Take the top k indices\n",
    "    top_indices = sorted_indices_desc[:k_features]\n",
    "\n",
    "    selected_columns_X_train = X_train.iloc[:, top_indices]\n",
    "    selected_columns_X_test = X_test.iloc[:, top_indices]\n",
    "\n",
    "    return selected_columns_X_train, selected_columns_X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_k_features_from_ANOVA(model, *args):\n",
    "    X_train = args[0]\n",
    "    original_n_features = len(X_train.columns)\n",
    "\n",
    "    train_acc_dict = {}  # 0 is a dummy accuracy for k=0 features\n",
    "    test_acc_dict = {}\n",
    "    train_test_dataset = {}\n",
    "\n",
    "    for k in range(1, original_n_features + 1):\n",
    "        train_test_dataset_after_ANOVA = get_df_with_top_k_features(k, *args)\n",
    "        train_accuracy, test_accuracy = model(*train_test_dataset_after_ANOVA)\n",
    "        train_test_dataset[k] = train_test_dataset_after_ANOVA\n",
    "        train_acc_dict[k] = train_accuracy\n",
    "        test_acc_dict[k] = test_accuracy\n",
    "        print(\n",
    "            f\"k: {k}, train_accuracy: {train_accuracy}, test_accuracy: {test_accuracy}\"\n",
    "        )\n",
    "\n",
    "    # Find k that gives the highest accuracy\n",
    "    best_train_k = max(train_acc_dict, key=train_acc_dict.get)\n",
    "    best_test_k = max(test_acc_dict, key=test_acc_dict.get)\n",
    "\n",
    "    print(f\"\\033[96mBest k for train_accuracy:\\033[00m {best_train_k}\")\n",
    "    print(f\"\\033[96mBest k for test_accuracy:\\033[00m {best_test_k}\")\n",
    "\n",
    "    plot_ANOVA_test_graph(train_acc_dict, test_acc_dict)\n",
    "\n",
    "    return train_test_dataset[best_test_k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using ANOVA test for feature selection on our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "def train_and_evaluate_svm(*args):\n",
    "    params = {\"kernel\": \"sigmoid\"}\n",
    "    svm_model = SVC(**params)\n",
    "    train_accuracy, test_accuracy = dc.train_and_evaluate_model(svm_model, *args)\n",
    "    return train_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: remove this code chuck\n",
    "train_test_dataset = preprocess(df)\n",
    "train_test_smote_dataset = preprocess_with_SMOTE(df)\n",
    "\n",
    "svm_param = {\"kernel\": \"sigmoid\"}\n",
    "svm_model_result = train_and_evaluate_svm(*train_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_dataset = preprocess(df)\n",
    "train_test_smote_dataset = preprocess_with_SMOTE(df)\n",
    "\n",
    "# Example of using the function\n",
    "# best_train_test_df = find_best_k_features_from_ANOVA(train_and_evaluate_svm, *train_test_dataset)\n",
    "\n",
    "# Since we have class imbalance, we will use the dataset with SMOTE\n",
    "best_train_test_smote_df = find_best_k_features_from_ANOVA(\n",
    "    train_and_evaluate_svm, *train_test_smote_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation:  \n",
    "Using the ANOVA test feature selection method, we see that using ANOVA F-test to choose the best k features for different k values do not seem to affect the accuracy of the model, since the accuracy lays between 0.915 to 0.95 for k values 1-64.\n",
    "\n",
    "Hence, using ANOVA test feature selection is not suitable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measures the reduction in entropy from transforming a dataset in some way. It is often used in training decision trees. Information Gain can be applied to feature selection by evaluating the mutual information between each feature and the target variable. Features that have higher mutual information with the target variable are considered more informative and are thus selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "def information_gain(X, y):\n",
    "    mi_scores = mutual_info_classif(X, y, discrete_features=\"auto\")\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n",
    "\n",
    "\n",
    "def correlation_selection(X, y):\n",
    "    if isinstance(y, pd.Series):\n",
    "        y = y.to_frame(name=\"Target\")\n",
    "    elif isinstance(y, pd.DataFrame):\n",
    "        y.columns = [\"Target\"]\n",
    "\n",
    "    df_combined = pd.concat([X, y], axis=1)\n",
    "    correlation_matrix = df_combined.corr()\n",
    "    correlation_w_target = correlation_matrix[\"Target\"].drop(\"Target\")\n",
    "    return correlation_w_target.abs().sort_values(ascending=False)\n",
    "\n",
    "\n",
    "def plot_feature_performance(X, y, score_series, model, max_features=None):\n",
    "    f1_results = []\n",
    "    recall_results = []\n",
    "    feature_counts = []\n",
    "\n",
    "    if not max_features:\n",
    "        max_features = len(score_series)\n",
    "\n",
    "    for i in range(1, max_features + 1):\n",
    "        top_features = score_series.nlargest(i).index\n",
    "        X_selected = X[top_features]\n",
    "\n",
    "        # Cross-validation F1 and recall scores\n",
    "        f1 = cross_val_score(model, X_selected, y, cv=5, scoring=\"f1\")\n",
    "        recall = cross_val_score(model, X_selected, y, cv=5, scoring=\"recall\")\n",
    "\n",
    "        f1_results.append(f1.mean())\n",
    "        recall_results.append(recall.mean())\n",
    "        feature_counts.append(i)\n",
    "\n",
    "    # Plotting the F1-score and recall results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(\n",
    "        feature_counts,\n",
    "        f1_results,\n",
    "        marker=\"o\",\n",
    "        linestyle=\"-\",\n",
    "        markersize=8,\n",
    "        label=\"F1 Score\",\n",
    "    )\n",
    "    plt.plot(\n",
    "        feature_counts,\n",
    "        recall_results,\n",
    "        marker=\"o\",\n",
    "        linestyle=\"-\",\n",
    "        markersize=8,\n",
    "        label=\"Recall\",\n",
    "    )\n",
    "    plt.xlabel(\"Number of Features\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(\"Model Performance vs. Number of Features\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using the function\n",
    "train_test_dataset = preprocess(df)\n",
    "train_test_smote_dataset = preprocess_with_SMOTE(df)\n",
    "\n",
    "svm_model = SVC(kernel=\"sigmoid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper Methods\n",
    "Wrappers require some method to search the space of all possible subsets of features, assessing their quality by learning and evaluating a classifier with that feature subset. The feature selection process is based on a specific machine learning algorithm we are trying to fit on a given dataset. It follows a greedy search approach by evaluating all the possible combinations of features against the evaluation criterion. The wrapper methods usually result in better predictive accuracy than filter methods.\n",
    "\n",
    "Wrapper methods:\n",
    "1. Forward Feature Selection\n",
    "2. Backward Feature Elimination\n",
    "3. Exhaustive Feature Selection\n",
    "4. Recursive Feature Elimination\n",
    "\n",
    "The wrapper method for feature selection is a technique used to identify the most significant features for a predictive model. It works by starting with an empty set and adding features one by one, each time choosing the feature that, when added, most improves the model's F1 score - our desirable metric. This approach is iterative and selects features based on their contribution to the model's predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "def forward_feature_selection(model, X_train, y_train, X_test, y_test):\n",
    "    selected_features = []\n",
    "    best_f1 = 0\n",
    "    features = list(X_train.columns)\n",
    "\n",
    "    for _ in range(len(features)):\n",
    "        f1_scores = []\n",
    "        for feature in features:\n",
    "            if feature not in selected_features:\n",
    "                temp_features = selected_features + [feature]\n",
    "                model.fit(X_train[temp_features], y_train)\n",
    "                y_pred = model.predict(X_test[temp_features])\n",
    "                f1 = f1_score(y_test, y_pred)\n",
    "                f1_scores.append((feature, f1))\n",
    "\n",
    "        # Find the best feature and its f1 score\n",
    "        f1_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        best_feature, best_feature_f1 = f1_scores[0]\n",
    "\n",
    "        if best_feature_f1 > best_f1:\n",
    "            print(f\"Adding {best_feature} improved F1 to {best_feature_f1}\")\n",
    "            best_f1 = best_feature_f1\n",
    "            selected_features.append(best_feature)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    print(\"Selected features:\", selected_features)\n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using the function\n",
    "train_test_dataset = preprocess(df)\n",
    "train_test_smote_dataset = preprocess_with_SMOTE(df)\n",
    "\n",
    "svm_model = SVC(kernel=\"sigmoid\")\n",
    "forward_feature_selection(svm_model, train_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedded Methods\n",
    "These methods encompass the benefits of both the wrapper and filter methods by including interactions of features but also maintaining reasonable computational costs. Embedded methods are iterative in the sense that takes care of each iteration of the model training process and carefully extract those features which contribute the most to the training for a particular iteration.\n",
    "\n",
    "Example techniques:\n",
    "1. LASSO Regularisation (L1)\n",
    "2. Random Forest Importance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Methods\n",
    "1. Principal Component Analysis (PCA): \n",
    "    - Reduces the dimensionality of the data by transforming it into a new set of features that are linear combinations of the original features.\n",
    "2. Independent Component Analysis (ICA): \n",
    "    - Similar to PCA, but focuses on making the components statistically independent rather than uncorrelated.\n",
    "3. Factor Analysis: \n",
    "    - Identifies underlying relationships between features by modeling the observed variables as linear combinations of potential factors.\n",
    "4. t-Distributed Stochastic Neighbor Embedding (t-SNE): \n",
    "    - Primarily used for visualization but can help in understanding the structure of the data for feature selection.\n",
    "5. Autoencoders: \n",
    "    - Uses neural networks to learn a compressed representation of the data, which can be used for feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
